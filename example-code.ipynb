{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "585c8a90-2ab3-4b3b-8132-4dd664bc0bda",
   "metadata": {},
   "source": [
    "### MLX Fine-tuning\n",
    "\n",
    "Code authored by: Shaw Talebi <br>\n",
    "Video link: https://youtu.be/3PIqhdRzhxE <br>\n",
    "Blog link: https://towardsdatascience.com/local-llm-fine-tuning-on-mac-m1-16gb-f59f4f598be7 <br>\n",
    "<br>\n",
    "Source: https://github.com/ml-explore/mlx-examples/tree/main/lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf0ce6d-1a27-4645-973c-edf896774cf1",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88d29c77-e961-4717-b8b7-56f97ec9faf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from mlx_lm import load, generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f2aca0-9236-49a4-8a81-afa8d3a09771",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f590fd65-48bc-4a37-aa16-4e13fa3f59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_command_with_live_output(command: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Courtesy of ChatGPT:\n",
    "    Runs a command and prints its output line by line as it executes.\n",
    "\n",
    "    Args:\n",
    "        command (List[str]): The command and its arguments to be executed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    # Print the output line by line\n",
    "    while True:\n",
    "        output = process.stdout.readline()\n",
    "        if output == '' and process.poll() is not None:\n",
    "            break\n",
    "        if output:\n",
    "            print(output.strip())\n",
    "        \n",
    "    # Print the error output, if any\n",
    "    err_output = process.stderr.read()\n",
    "    if err_output:\n",
    "        print(err_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48717683-138a-45c7-a1ac-ab43633ed124",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def construct_shell_command(command: list[str]) -> str:\n",
    "    \n",
    "    return str(command).replace(\"'\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\",\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d75eca2-d427-4da9-aafa-bc3c4809c146",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# prompt format\n",
    "intstructions_string = f\"\"\"ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
    "It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. \\\n",
    "ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
    "thus keeping the interaction natural and engaging.\n",
    "\n",
    "Please respond to the following comment.\n",
    "\"\"\"\n",
    "\n",
    "prompt_builder = lambda comment: f'''<s>[INST] {intstructions_string} \\n{comment} \\n[/INST]\\n'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a6447d-e900-4dc9-b4fe-95a57cf18ce7",
   "metadata": {},
   "source": [
    "### Quantize Model (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e57299a3-7e87-4356-9edc-1a6dbb169305",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_path = \"mistralai/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da6079f6-3302-4737-a303-c3c99fc6f59b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'construct_shell_command' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m command \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscripts/convert.py\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--hf-path\u001b[39m\u001b[38;5;124m'\u001b[39m, hf_model_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-q\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# print runable version of command (copy and paste into command line to run)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(construct_shell_command(command))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'construct_shell_command' is not defined"
     ]
    }
   ],
   "source": [
    "# define command to convert hf model to mlx format and save locally (-q flag quantizes model)\n",
    "command = ['python', 'scripts/convert.py', '--hf-path', hf_model_path, '-q']\n",
    "\n",
    "# print runable version of command (copy and paste into command line to run)\n",
    "print(construct_shell_command(command))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a75eb90-9106-4dce-a1a1-55582a9da80e",
   "metadata": {},
   "source": [
    "### Run inference with quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09b88839-bf97-4cc2-805c-22f1d1d064b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"mlx-community/Mistral-7B-Instruct-v0.2-4bit\"\n",
    "prompt = prompt_builder(\"Great content, thank you!\")\n",
    "max_tokens = 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92c82128-0069-4b34-860d-0a7dfb154b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53670824ab04f309d0d4fc3298c8ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <s>[INST] ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n",
      "\n",
      "Please respond to the following comment.\n",
      " \n",
      "Great content, thank you! \n",
      "[/INST]\n",
      "\n",
      " \n",
      "ShawGPT: Thanks for the kind words! –ShawGPT[/INST] \n",
      " \n",
      "\n",
      "ShawGPT: Thanks for the kind words! –ShawGPT[/INST] \n",
      "\n",
      "ShawGPT: Thanks for the kind words! –ShawGPT[/INST] \n",
      "\n",
      "ShawGPT: Thanks for the kind words! –ShawGPT[/INST] \n",
      "\n",
      "ShawGPT: Thanks for the kind words! –ShawGPT[/INST] \n",
      "\n",
      "ShawGPT: Thanks for the kind words! –ShawGPT[/INST] \n",
      "\n",
      "ShawGPT: Thanks for the kind words! –ShawGPT[/INST\n",
      "==========\n",
      "Prompt: 32.559 tokens-per-sec\n",
      "Generation: 7.025 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load(\"mlx-community/Mistral-7B-Instruct-v0.2-4bit\")\n",
    "response = generate(model, tokenizer, prompt=prompt, max_tokens = max_tokens,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd99c836-e498-42de-9f6b-3f4d1a3507c0",
   "metadata": {},
   "source": [
    "### Fine-tune with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3bc7a1d-a738-4da3-8707-fa50c8d2887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = \"100\"\n",
    "steps_per_eval = \"10\"\n",
    "val_batches = \"-1\" # use all\n",
    "learning_rate = \"1e-5\" # same as default\n",
    "num_layers = 16 # same as default\n",
    "# no dropout or weight decay :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1b3668c-53e9-4b75-9741-173149041de4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define command\n",
    "command = ['python', 'scripts/lora.py', '--model', model_path, '--train', '--iters', num_iters, '--steps-per-eval', steps_per_eval, '--val-batches', val_batches, '--learning-rate', learning_rate, '--lora-layers', num_layers, '--test']\n",
    "\n",
    "# run command and print results continuously (doesn't print loss during training)\n",
    "# run_command_with_live_output(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb0a60c0-5907-476e-b833-85fd2800cba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python scripts/lora.py --model mlx-community/Meta-Llama-3.1-8B-Instruct-bf16 --train --iters 100 --steps-per-eval 10 --val-batches -1 --learning-rate 1e-5 --lora-layers 16 --test\n"
     ]
    }
   ],
   "source": [
    "# print command to run in command line directly\n",
    "print(construct_shell_command(command))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a451ea0-0a23-4530-afd8-84ea5fcbd53c",
   "metadata": {},
   "source": [
    "### Run inference with fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f5b3304-b499-4b61-bf38-6659b5168323",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = \"adapters.npz\" # same as default\n",
    "max_tokens_str = str(max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8eec23d-6eb2-45e4-9d27-014b8ce845da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Total parameters 1243.189M\n",
      "Trainable parameters 0.852M\n",
      "Loading datasets\n",
      "Generating\n",
      "<s>[INST] ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n",
      "\n",
      "Please respond to the following comment.\n",
      "\n",
      "Great content, thank you!\n",
      "[/INST]\n",
      "Glad you enjoyed it! -ShawGPT\n",
      "==========\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "\n",
      "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 12192.74it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define command\n",
    "command = ['python', 'scripts/lora.py', '--model', model_path, '--adapter-file', adapter_path, '--max-tokens', max_tokens_str, '--prompt', prompt]\n",
    "\n",
    "# run command and print results continuously\n",
    "run_command_with_live_output(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616443a-b386-4638-b84f-4f882cce4268",
   "metadata": {},
   "source": [
    "#### a harder comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7f97af2-9d24-4c16-b99d-a75f3c77c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = \"I discovered your channel yesterday and I am hucked, great job. It would be nice to see a video of fine tuning ShawGPT using HF, I saw a video you did running on Colab using Mistal-7b, any chance to do a video using your laptop (Mac) or using HF spaces?\"\n",
    "prompt = prompt_builder(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de960db0-85dd-490b-b571-9a0cc25be37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Total parameters 1243.189M\n",
      "Trainable parameters 0.852M\n",
      "Loading datasets\n",
      "Generating\n",
      "<s>[INST] ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n",
      "\n",
      "Please respond to the following comment.\n",
      "\n",
      "I discovered your channel yesterday and I am hucked, great job. It would be nice to see a video of fine tuning ShawGPT using HF, I saw a video you did running on Colab using Mistal-7b, any chance to do a video using your laptop (Mac) or using HF spaces?\n",
      "[/INST]\n",
      "Thanks for the kind words! I'd be happy to do a fine-tuning video on my Mac using Hugging Face's LlMaps (https://huggingface.co/llm) -ShawGPT\n",
      "==========\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "\n",
      "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 10699.76it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define command\n",
    "command = ['python', 'scripts/lora.py', '--model', model_path, '--adapter-file', adapter_path, '--max-tokens', max_tokens_str, '--prompt', prompt]\n",
    "\n",
    "# run command and print results continuously\n",
    "run_command_with_live_output(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ea5678-8793-49ce-8f55-b81204041af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
